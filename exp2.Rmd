---
title: "Exp 2"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
 \usepackage{booktabs}
 \usepackage{longtable}
 \usepackage{array}
 \usepackage{multirow}
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
---

# Setup

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H',
                      echo=TRUE, warning=FALSE, message=FALSE)

```
```{r , include=FALSE}
library(tidyverse)
library(estimatr)
library(hexbin)
library(ggplot2)
library(readr)
library(zoo)

th <- theme_classic()
theme_set(th)
```

Load subject data

```{r echo=TRUE}

model_data <- read_csv("data/isr_inertia_480_attention.csv") %>%
  mutate(compute = rowSums(dplyr::select(., starts_with("tracker")))) %>%
  dplyr::select(-c(starts_with("tracker"))) %>%
  rename(frame = t, scene = trial)

min_frame = 36 # beginning is unreliable for difficulty
max_frame = 480

# TODO do we control for td_acc == 1
subject_data <- read_csv("data/parsed_trials.csv") %>%
  dplyr::select(-c(WID)) %>%
  rename(diff = difficulty) %>% # either choose raw presses or recreated continuous difficulty
  filter(frame >= min_frame,
         frame <= min(max_frame, max(frame) - lead_n)) # making sure not to have NAs at the end

```

Screning BAD subjects

```{r}

perf_thresh = 3.0
scenes_pressed_thresh = 0.5

# arbitrary threshold for how many diff a subject should do within a scene
# TODO make less arbitrary
presses_threshold_min = 0
presses_threshold_max = 1000

subject_performance <- subject_data %>%
  group_by(ID, scene) %>%
  mutate(scene_pressed = sum(presses) > 0) %>%
  ungroup() %>%
  group_by(ID) %>%
  summarise(td_acc_mu = mean(td_acc),
            n = n(),
            scenes_pressed = sum(scene_pressed)/n,
            td_acc_se = sd(td_acc)/sqrt(n/3),
            ID_passed = abs(td_acc_mu - 0.5) > perf_thresh*td_acc_se & scenes_pressed > scenes_pressed_thresh)

subject_presses <- subject_data %>%
  group_by(scene, ID) %>%
  summarize(presses_scene_sum = sum(presses),
            pressed_scene = presses_scene_sum > presses_threshold_min &
                            presses_scene_sum < presses_threshold_max) %>%
  ungroup()

left_join(subject_presses, subject_performance) %>%
  filter(ID_passed) %>%
  ggplot() +
  geom_histogram(aes(x=presses_scene_sum)) +
  facet_wrap(vars(ID))

# TODO? td_pass_scene = td_acc == 1.0

good_subject_data <- left_join(subject_presses, subject_performance) %>%
  filter(ID_passed, pressed_scene) %>%
  left_join(subject_data) %>%
  select(-c(ID_passed, pressed_scene))
```

```{r}
left_join(good_subject_data, model_data) %>%
  filter(scene == 3) %>%
  ggplot() +
  geom_line(aes(x=frame, y=compute, color="compute")) +
  geom_line(aes(x=frame, y=diff*10, color=factor(ID))) +
  facet_wrap(vars(scene))
```


Let's look at scatter plot of compute vs. difficulty:
```{r}
sum_compute_diff <- left_join(good_subject_data, model_data) %>%
  group_by(ID) %>%
  mutate(sum_diff = sum(diff)) %>%
  group_by(scene, ID) %>%
  mutate(scene_sum_diff = sum(diff)/sum_diff) %>%
  group_by(scene) %>%
  mutate(scene_sum_comp = sum(compute)) %>%
  ungroup() %>%
  group_by(scene) %>%
  mutate(scene_sum_diff_mu = mean(scene_sum_diff)) %>%
  ungroup()

#individual correlations
correlations <- sum_compute_diff %>%
  group_by(ID) %>%
  summarise(corr=cor(scene_sum_diff, scene_sum_comp, method="pearson"),
            p.value=cor.test(scene_sum_diff, scene_sum_comp, method="pearson")$p.value) %>%
  ungroup()
correlations

#average subject correlation
sum_compute_diff %>%
  summarise(corr=cor(scene_sum_comp, scene_sum_diff_mu),
            p.value=cor.test(scene_sum_comp, scene_sum_diff_mu)$p.value)


sum_compute_diff %>%
  ggplot() +
  geom_point(aes(x=scene_sum_comp, y=scene_sum_diff, color=factor(ID))) +
  geom_point(shape=23, size=5, aes(x=scene_sum_comp, y=scene_sum_diff_mu, color="average Participant"))
```

Data smoothing

```{r}
model_smoothing <- 36
subject_smoothing <- 36
lead_n <- 20

max_smoothing <- max(model_smoothing, subject_smoothing)

# s_diff - smooth difficulty
subject_smooth <- good_subject_data %>%
  group_by(ID) %>%
  summarise(diff_sum = sum(diff)) %>%
  right_join(good_subject_data, by="ID") %>%
  mutate(diff_norm = diff/diff_sum) %>%
  ungroup() %>%
  select(ID, scene, frame, diff_norm)


# s_comp - smooth compute
model_smooth <- model_data %>%
  nest_by(scene) %>%
  mutate(s_comp_xy = list(with(data,
                          ksmooth(frame, compute, kernel = "normal", bandwidth = model_smoothing)))) %>%
  mutate(s_comp = list(s_comp_xy$y)) %>%
  unnest(cols = c(data, s_comp)) %>%
  dplyr::select(-c(s_comp_xy)) %>%
  filter(frame >= max_smoothing,
         frame <= max(frame) - max_smoothing) %>%
  group_by(scene) %>%
  mutate(comp = (s_comp - min(s_comp))/(max(s_comp) - min(s_comp)),
         comp_z = scale(s_comp),
         comp_cum = cumsum(comp),
         comp_cum = comp_cum/max(comp_cum)) %>%
  ungroup()
```



Bootstrapping prep

```{r}
# number of subjects for one bootstrap operation
n_subjects = 20

# sample
sample_subject_data <- function() {
  sampled_IDs = sample(unique(subject_smooth$ID), n_subjects, replace=TRUE)
  
  data <- subject_smooth %>%
    filter(ID == sampled_IDs[1])
  
  for (id in sampled_IDs[2:n_subjects]) {
    data <- subject_smooth %>%
      filter(ID == id) %>%
      bind_rows(data)
  }
  data  
}

# make one average subject by aggregating subject data
agg_subject_data <- function(data) {
  data %>%
    group_by(scene, frame) %>%
    summarise(diff_agg = mean(diff_norm)) %>%
    ungroup() %>%
    
    nest_by(scene) %>%
    mutate(diff_s_xy = list(with(data,
                                 ksmooth(frame, diff_agg, kernel = "normal", bandwidth = subject_smoothing)))) %>%
    mutate(diff_s = list(diff_s_xy$y)) %>%
    unnest(cols = c(data, diff_s)) %>%
    dplyr::select(-c(diff_s_xy)) %>%
    
    group_by(scene) %>%
    mutate(diff_s = lead(diff_s, lead_n)) %>%
    filter(frame >= max_smoothing,
           frame <= max(frame) - max_smoothing - lead_n) %>%
    ungroup() %>%
    
    group_by(scene) %>%
    mutate(diff_agg = (diff_s - min(diff_s))/(max(diff_s)-min(diff_s)),
           diff_z = scale(diff_s),
           diff_cum = cumsum(diff_agg),
           diff_cum = diff_cum/max(diff_cum)) %>%
    ungroup()
}
```

Testing bootstrap prep

```{r}
s <- sample_subject_data()
a <- agg_subject_data(s)

left_join(a, model_smooth) %>%
  # filter(scene == 3) %>%
  ggplot(aes(x=frame)) +
  geom_line(aes(y=comp_z, color="compute")) +
  geom_line(aes(y=diff_z, color="mean difficulty")) +
  facet_wrap(vars(scene))

left_join(a, model_smooth) %>%
  ggplot() +
  geom_density_2d_filled(aes(x=comp, y=diff_agg))

```


Bootstrapping

```{r}

# we run the outer loop 1000 times
n_iters <- 100
n_scenes <- max(subject_smooth$scene)

bootstrap <- function(data) {
  s <- sample_subject_data()
  a <- agg_subject_data(s)
  combined_data <- left_join(a, model_smooth)
  
  # calculating the mean correlation along the scenes
  # for our hypothesis
  h1 <- combined_data %>%
    group_by(scene) %>%
    summarise(h1_corr = cor(comp_z, diff_z)) %>%
    summarise(h1_corr = mean(h1_corr)) %>%
    pull(h1_corr)
  
  scenes_permuted <- sample(n_scenes, n_scenes)
  null_vector = vector(,n_scenes)
  for (i in seq_along(scenes_permuted)) {
    x <- combined_data %>%
      filter(scene == i) %>%
      pull(comp_z)
    
    y <- combined_data %>%
      filter(scene == scenes_permuted[i]) %>%
      pull(diff_z)
    
    null_vector[i] <- cor(x,y)
  }
  null <- mean(null_vector)
  
  data.frame(h1 = h1, null = null)
}

correlation_data <- bootstrap()
for (i in seq(n_iters-1)) {
  correlation_data <- correlation_data %>%
    bind_rows(bootstrap())
}

correlation_data %>%
  ggplot() +
  geom_histogram(aes(x=h1, color="h1")) +
  geom_histogram(aes(x=null, color="null"))
```

```{r}
left_join(subject_smooth, model_smooth) %>%
  ggplot(aes(x=frame)) +
  geom_line(aes(y=comp, color="compute")) +
  geom_line(aes(y=diff_agg, color="mean difficulty")) +
  geom_linerange(aes(ymin=conf_low, ymax=conf_high)) +
  facet_wrap(vars(scene))

full_data <- left_join(subject_smooth, model_smooth)
```


```{r}

# get 40,000 correlation by bootstrapping
# 1) sample subjects with replacement
# 2) do the rest of analysis
# 3) repeat 1000 times

correlations <- full_data %>%
  group_by(scene) %>%
  dplyr::summarise(corr=cor(comp, diff_agg, method="pearson")) %>%
  ungroup()
correlations

cor.test(full_data$diff_agg, full_data$comp, method="pearson")
```

Getting the null hypothesis distribution

```{r}
n_scenes = max(full_data$scene)
n_reps = 2

get_null_r <- function() {
  # choosing randomly to correlate
  scenes <- sample(n_scenes, 2)
  x <- full_data %>%
    filter(scene == scenes[1]) %>%
    select(comp)
  y <- full_data %>%
    filter(scene == scenes[2]) %>%
    select(diff_agg)
  cor(x, y)
}

get_h1_r <- function() {
  rep(1, 40)
}

correlation_data <- data.frame(null = replicate(n_scenes*n_reps, get_null_r()),
                               h1 = replicate(n_reps, get_h1_r()))

```


Permutation test
```{r}
nperm <- 1000

x <- full_data$comp
y <- full_data$diff_agg

# TODO per scene (make sure it's not the right scene when permuted)
r.obs <- cor(x, y)
P.par <- cor.test(x, y)$p.value
r.per <- replicate(nperm, expr = cor (x, sample(y)))
summary(r.per)

# data.frame(r.per = r.per, subjects = replicate(nperm, sample(correlations$corr))) %>%
#   ggplot() +
#   geom_histogram(aes(x=r.per)) +
#   geom_histogram(aes(x=subjects))

hist(correlations$corr, xlim = c(-1,1))
abline(v = r.obs, col = 'red')
abline(v = r.per, col = 'green')
abline(v = mean(correlations$corr), col = 'blue')

```