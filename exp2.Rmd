---
title: "Exp 2"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
 \usepackage{booktabs}
 \usepackage{longtable}
 \usepackage{array}
 \usepackage{multirow}
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
---

# Setup

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H',
                      echo=TRUE, warning=FALSE, message=FALSE)

```
```{r , include=FALSE}
library(tidyverse)
library(estimatr)
library(hexbin)
library(ggplot2)
library(readr)
library(zoo)

th <- theme_classic()
theme_set(th)
```

Load subject data

```{r echo=TRUE}

model_data <- read_csv("data/isr_inertia_480_attention.csv") %>%
  mutate(compute = rowSums(dplyr::select(., starts_with("tracker")))) %>%
  dplyr::select(-c(starts_with("tracker"))) %>%
  rename(frame = t, scene = trial)

min_frame = 36 # beginning is unreliable for difficulty
max_frame = 480

# TODO do we control for td_acc == 1
subject_data <- read_csv("data/parsed_trials.csv") %>%
  dplyr::select(-c(WID)) %>%
  rename(diff = difficulty) %>% # either choose raw presses or recreated continuous difficulty
  filter(frame >= min_frame,
         frame <= min(max_frame, max(frame) - lead_n)) # making sure not to have NAs at the end

```

Screning BAD subjects

```{r}

perf_thresh = 3.0
scenes_pressed_thresh = 0.5

# arbitrary threshold for how many diff a subject should do within a scene
# TODO make less arbitrary
presses_threshold_min = 0
presses_threshold_max = 1000

subject_performance <- subject_data %>%
  group_by(ID, scene) %>%
  mutate(scene_pressed = sum(presses) > 0) %>%
  ungroup() %>%
  group_by(ID) %>%
  summarise(td_acc_mu = mean(td_acc),
            n = n(),
            scenes_pressed = sum(scene_pressed)/n,
            td_acc_se = sd(td_acc)/sqrt(n/3),
            ID_passed = abs(td_acc_mu - 0.5) > perf_thresh*td_acc_se & scenes_pressed > scenes_pressed_thresh)

subject_presses <- subject_data %>%
  group_by(scene, ID) %>%
  summarize(presses_scene_sum = sum(presses),
            pressed_scene = presses_scene_sum > presses_threshold_min &
                            presses_scene_sum < presses_threshold_max)

left_join(subject_presses, subject_performance) %>%
  filter(ID_passed) %>%
  ggplot() +
  geom_histogram(aes(x=presses_scene_sum)) +
  facet_wrap(vars(ID))

# TODO? td_pass_scene = td_acc == 1.0

good_subject_data <- left_join(subject_diff, subject_performance) %>%
  filter(ID_passed, pressed_scene) %>%
  left_join(subject_data) %>%
  select(-c(ID_passed, pressed_scene))
```

```{r}
left_join(good_subject_data, model_data) %>%
  filter(scene == 3) %>%
  ggplot() +
  geom_line(aes(x=frame, y=compute, color="compute")) +
  geom_line(aes(x=frame, y=diff*10, color=factor(ID))) +
  facet_wrap(vars(scene))
```


Let's look at scatter plot of compute vs. difficulty:
```{r}
sum_compute_diff <- left_join(good_subject_data, model_data) %>%
  group_by(ID) %>%
  mutate(sum_diff = sum(diff)) %>%
  group_by(scene, ID) %>%
  mutate(scene_sum_diff = sum(diff)/sum_diff) %>%
  group_by(scene) %>%
  mutate(scene_sum_comp = sum(compute)) %>%
  ungroup() %>%
  group_by(scene) %>%
  mutate(scene_sum_diff_mu = mean(scene_sum_diff)) %>%
  ungroup()

#individual correlations
correlations <- sum_compute_diff %>%
  group_by(ID) %>%
  summarise(corr=cor(scene_sum_diff, scene_sum_comp, method="pearson"),
            p.value=cor.test(scene_sum_diff, scene_sum_comp, method="pearson")$p.value) %>%
  ungroup()
correlations

#average subject correlation
sum_compute_diff %>%
  summarise(corr=cor(scene_sum_comp, scene_sum_diff_mu),
            p.value=cor.test(scene_sum_comp, scene_sum_diff_mu)$p.value)


sum_compute_diff %>%
  ggplot() +
  geom_point(aes(x=scene_sum_comp, y=scene_sum_diff, color=factor(ID))) +
  geom_point(shape=23, size=5, aes(x=scene_sum_comp, y=scene_sum_diff_mu, color="average Participant"))
```

Data smoothing

```{r}
model_smoothing <- 36
subject_smoothing <- 36
lead_n <- 24

max_smoothing <- max(model_smoothing, subject_smoothing)

# s_diff - smooth difficulty
subject_smooth <- good_subject_data %>%
  group_by(ID, scene) %>%
  summarise(total_diff = sum(diff)) %>%
  group_by(ID) %>%
  summarise(total_diff_mu = mean(total_diff)) %>%
  right_join(good_subject_data, by="ID") %>%
  mutate(diff_rs = rollsum(diff, subject_smoothing, fill = 0),
         diff_agg = diff/total_diff_mu) %>%
  ungroup() %>%
  group_by(scene, frame) %>%
  summarise(diff_agg_se = sd(diff_agg)/sqrt(n()),
            diff_agg = mean(diff_agg),
            conf_low = diff_agg - 3 * diff_agg_se,
            conf_high = diff_agg + 3 * diff_agg_se) %>%
  ungroup() %>%
  nest_by(scene) %>%
  mutate(diff_agg_s_xy = list(with(data,
                            ksmooth(frame, diff_agg, kernel = "normal", bandwidth = subject_smoothing)))) %>%
  mutate(diff_agg_s = list(diff_agg_s_xy$y)) %>%
  unnest(cols = c(data, diff_agg_s)) %>%
  dplyr::select(-c(diff_agg_s_xy)) %>%
  group_by(scene) %>%
  mutate(diff_agg = lead(diff_agg, lead_n)) %>%
  filter(frame >= max_smoothing,
         frame <= max(frame) - max_smoothing - lead_n) %>%
  mutate(diff_agg = diff_agg_s/max(diff_agg_s)) %>%
  ungroup()


# s_comp - smooth compute
model_smooth <- model_data %>%
  nest_by(scene) %>%
  mutate(s_comp_xy = list(with(data,
                          ksmooth(frame, compute, kernel = "normal", bandwidth = model_smoothing)))) %>%
  mutate(s_comp = list(s_comp_xy$y)) %>%
  unnest(cols = c(data, s_comp)) %>%
  dplyr::select(-c(s_comp_xy)) %>%
  filter(frame >= max_smoothing,
         frame <= max(frame) - max_smoothing) %>%
  group_by(scene) %>%
  mutate(comp = s_comp/max(s_comp)) %>%
  ungroup()
```


```{r}
left_join(subject_smooth, model_smooth) %>%
  ggplot(aes(x=frame)) +
  geom_line(aes(y=comp, color="compute")) +
  geom_line(aes(y=diff_agg, color="mean difficulty")) +
  geom_linerange(aes(ymin=conf_low, ymax=conf_high)) +
  facet_wrap(vars(scene))

full_data <- left_join(subject_smooth, model_smooth)
```


```{r}

# get 40,000 correlation by bootstrapping
# 1) sample subjects with replacement
# 2) do the rest of analysis
# 3) repeat 1000 times

correlations <- full_data %>%
  group_by(scene) %>%
  dplyr::summarise(corr=cor(comp, diff_agg, method="pearson")) %>%
  ungroup()
correlations

cor.test(full_data$diff_agg, full_data$comp, method="pearson")
```

Getting the null hypothesis distribution

```{r}
n_scenes = max(full_data$scene)
n_reps = 2

get_null_r <- function() {
  # choosing randomly to correlate
  scenes <- sample(n_scenes, 2)
  x <- full_data %>%
    filter(scene == scenes[1]) %>%
    select(comp)
  y <- full_data %>%
    filter(scene == scenes[2]) %>%
    select(diff_agg)
  cor(x, y)
}

get_h1_r <- function() {
  rep(1, 40)
}

correlation_data <- data.frame(null = replicate(n_scenes*n_reps, get_null_r()),
                               h1 = replicate(n_reps, get_h1_r()))

```


Permutation test
```{r}
nperm <- 1000

x <- full_data$comp
y <- full_data$diff_agg

# TODO per scene (make sure it's not the right scene when permuted)
r.obs <- cor(x, y)
P.par <- cor.test(x, y)$p.value
r.per <- replicate(nperm, expr = cor (x, sample(y)))
summary(r.per)

# data.frame(r.per = r.per, subjects = replicate(nperm, sample(correlations$corr))) %>%
#   ggplot() +
#   geom_histogram(aes(x=r.per)) +
#   geom_histogram(aes(x=subjects))

hist(correlations$corr, xlim = c(-1,1))
abline(v = r.obs, col = 'red')
abline(v = r.per, col = 'green')
abline(v = mean(correlations$corr), col = 'blue')

```