---
title: "Exp 2"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
 \usepackage{booktabs}
 \usepackage{longtable}
 \usepackage{array}
 \usepackage{multirow}
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
---

# Setup

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H',
                      echo=TRUE, warning=FALSE, message=FALSE)

```
```{r , include=FALSE}
library(tidyverse)
library(estimatr)
library(hexbin)
library(ggplot2)
library(readr)
library(zoo)

th <- theme_classic()
theme_set(th)
```

Load subject data

```{r echo=TRUE}
model_data <- read_csv("data/isr_inertia_480_attention.csv") %>%
  mutate(compute = rowSums(dplyr::select(., starts_with("tracker")))) %>%
  dplyr::select(-c(starts_with("tracker"))) %>%
  rename(frame = t, scene = trial)

min_frame = 1 # beginning may be unreliable for difficulty
max_frame = 480

# TODO do we control for td_acc == 1
subject_data <- read_csv("data/parsed_trials.csv") %>%
  dplyr::select(-c(WID)) %>%
  arrange(ID, scene, frame) %>%
  rename(diff = difficulty) %>% # either choose raw presses or recreated continuous difficulty
  filter(frame >= min_frame,
         frame <= max_frame) # making sure not to have NAs at the end

```

Screning BAD subjects

```{r}

perf_thresh = 3.0
scenes_pressed_thresh = 0.5

# arbitrary threshold for how many diff a subject should do within a scene
# TODO make less arbitrary
presses_threshold_min = 0
presses_threshold_max = 1000

subject_performance <- subject_data %>%
  group_by(ID, scene) %>%
  mutate(scene_pressed = sum(presses) > 0) %>%
  ungroup() %>%
  group_by(ID) %>%
  summarise(td_acc_mu = mean(td_acc),
            n = n(),
            scenes_pressed = sum(scene_pressed)/n,
            td_acc_se = sd(td_acc)/sqrt(n/3),
            ID_passed = abs(td_acc_mu - 0.5) > perf_thresh*td_acc_se & scenes_pressed > scenes_pressed_thresh)

subject_presses <- subject_data %>%
  group_by(scene, ID) %>%
  summarize(presses_scene_sum = sum(presses),
            pressed_scene = presses_scene_sum > presses_threshold_min &
                            presses_scene_sum < presses_threshold_max) %>%
  ungroup()

left_join(subject_presses, subject_performance) %>%
  filter(ID_passed) %>%
  ggplot() +
  geom_histogram(aes(x=presses_scene_sum)) +
  facet_wrap(vars(ID))

# TODO? td_pass_scene = td_acc == 1.0

good_subject_data <- left_join(subject_presses, subject_performance) %>%
  filter(ID_passed, pressed_scene) %>%
  left_join(subject_data) %>%
  select(-c(ID_passed, pressed_scene))
```

```{r}
left_join(good_subject_data, model_data) %>%
  filter(scene == 3) %>%
  ggplot() +
  geom_line(aes(x=frame, y=compute, color="compute")) +
  geom_line(aes(x=frame, y=diff*10, color=factor(ID))) +
  facet_wrap(vars(scene))
```


Let's look at scatter plot of compute vs. difficulty:
```{r}
sum_compute_diff <- left_join(good_subject_data, model_data) %>%
  group_by(ID) %>%
  mutate(sum_diff = sum(diff)) %>%
  group_by(scene, ID) %>%
  mutate(scene_sum_diff = sum(diff)/sum_diff) %>%
  group_by(scene) %>%
  mutate(scene_sum_comp = sum(compute)) %>%
  ungroup() %>%
  group_by(scene) %>%
  mutate(scene_sum_diff_mu = mean(scene_sum_diff)) %>%
  ungroup()

#individual correlations
correlations <- sum_compute_diff %>%
  group_by(ID) %>%
  summarise(corr=cor(scene_sum_diff, scene_sum_comp, method="pearson"),
            p.value=cor.test(scene_sum_diff, scene_sum_comp, method="pearson")$p.value) %>%
  ungroup()
correlations

#average subject correlation
sum_compute_diff %>%
  summarise(corr=cor(scene_sum_comp, scene_sum_diff_mu),
            p.value=cor.test(scene_sum_comp, scene_sum_diff_mu)$p.value)


sum_compute_diff %>%
  ggplot() +
  geom_point(aes(x=scene_sum_comp, y=scene_sum_diff, color=factor(ID))) +
  geom_point(shape=23, size=5, aes(x=scene_sum_comp, y=scene_sum_diff_mu, color="average Participant"))
```

Data smoothing

```{r}
model_smoothing <- 36
subject_smoothing <- 36
lead_n <- 20

max_smoothing <- max(model_smoothing, subject_smoothing)

# s_diff - smooth difficulty
subject_smooth <- good_subject_data %>%
  group_by(ID) %>%
  summarise(diff_sum = sum(diff)) %>%
  right_join(good_subject_data, by="ID") %>%
  mutate(diff_norm = diff/diff_sum) %>%
  ungroup() %>%
  select(ID, scene, frame, diff_norm)


# s_comp - smooth compute
model_smooth <- model_data %>%
  nest_by(scene) %>%
  mutate(s_comp_xy = list(with(data,
                          ksmooth(frame, compute, kernel = "normal", bandwidth = model_smoothing)))) %>%
  mutate(s_comp = list(s_comp_xy$y)) %>%
  unnest(cols = c(data, s_comp)) %>%
  dplyr::select(-c(s_comp_xy)) %>%
  filter(frame >= max_smoothing,
         frame <= max(frame) - max_smoothing) %>%
  group_by(scene) %>%
  mutate(comp = (s_comp - min(s_comp))/(max(s_comp) - min(s_comp)),
         comp_z = scale(s_comp),
         comp_cum = cumsum(comp),
         comp_cum = comp_cum/max(comp_cum)) %>%
  ungroup()
```



Bootstrapping prep

```{r}
# number of subjects for one bootstrap operation
n_subjects = 10

# sample
sample_subject_data <- function() {
  sampled_IDs = sample(unique(subject_smooth$ID), n_subjects, replace=TRUE)
  
  data <- subject_smooth %>%
    filter(ID == sampled_IDs[1])
  
  for (id in sampled_IDs[2:n_subjects]) {
    data <- subject_smooth %>%
      filter(ID == id) %>%
      bind_rows(data)
  }
  data  
}

# make one average subject by aggregating subject data
agg_subject_data <- function(data) {
  data %>%
    group_by(scene, frame) %>%
    summarise(diff_agg = mean(diff_norm), .groups="drop") %>%
    
    nest_by(scene) %>%
    mutate(diff_s_xy = list(with(data,
                                 ksmooth(frame, diff_agg, kernel = "normal", bandwidth = subject_smoothing)))) %>%
    mutate(diff_s = list(diff_s_xy$y)) %>%
    unnest(cols = c(data, diff_s)) %>%
    dplyr::select(-c(diff_s_xy)) %>%
    
    group_by(scene) %>%
    mutate(diff_s = lead(diff_s, lead_n)) %>%
    filter(frame >= max_smoothing,
           frame <= max(frame) - max_smoothing - lead_n) %>%
    ungroup() %>%
    
    group_by(scene) %>%
    # mutate(diff_agg = (diff_s - min(diff_s))/(max(diff_s)-min(diff_s)),
    #        diff_z = scale(diff_s),
    #        diff_cum = cumsum(diff_agg),
    #        diff_cum = diff_cum/max(diff_cum)) %>%
    mutate(diff_z = scale(diff_s)) %>%
    ungroup()
}
```

Testing bootstrap prep

```{r}
s <- sample_subject_data()
a <- agg_subject_data(s)

left_join(a, model_smooth) %>%
  # filter(scene == 3) %>%
  ggplot(aes(x=frame)) +
  geom_line(aes(y=comp_z, color="compute")) +
  geom_line(aes(y=diff_z, color="mean difficulty")) +
  facet_wrap(vars(scene))

left_join(a, model_smooth, by=c("scene", "frame")) %>%
  ggplot() +
  geom_density_2d_filled(aes(x=comp, y=diff_agg))

```


Bootstrapping

```{r}

# we run the outer loop 10000 times
n_iters <- 500
n_scenes <- max(subject_smooth$scene)

bootstrap <- function(data) {
  s <- sample_subject_data()
  a <- agg_subject_data(s)
  combined_data <- left_join(a, model_smooth, by=c("scene", "frame"))
  
  # calculating the mean correlation along the scenes
  # for our hypothesis
  
  h1_full <- cor(combined_data$comp_z, combined_data$diff_z)  
  
  h1_scene <- combined_data %>%
    group_by(scene) %>%
    summarise(h1_corr = cor(comp_z, diff_z), .groups="drop_last") %>%
    # summarise(h1_corr = mean(h1_corr), .groups="drop") %>%
    pull(h1_corr)
  
  # we want no ground truth mapping to remain
  while (TRUE) {
    scenes_permuted <- sample(n_scenes, n_scenes, replace=FALSE)
    if (all(1:n_scenes != scenes_permuted)) {
      break
    }
  }
  
  null_scene <- vector("numeric", n_scenes)
  
  x_full <- vector("numeric", length(combined_data))
  y_full <- combined_data$diff_z
  
  for (i in seq_along(scenes_permuted)) {
    x <-combined_data %>%
      filter(scene == scenes_permuted[i]) %>%
      pull(comp_z)
    
    n <- length(x)
    begin <- (i-1)*n+1
    end <- i*n
    
    x_full[begin:end] <- x
    
    y <- combined_data %>%
      filter(scene == i) %>%
      pull(diff_z)

    null_scene[i] <- cor(x,y)
  }
  
  null_full <- cor(x_full, y_full)
  
  data.frame(h1_full = h1_full,
             h1_scene = h1_scene,
             null_full = null_full,
             null_scene = null_scene)
}

correlation_data <- bootstrap()

for (i in seq(n_iters-1)) {
  correlation_data <- correlation_data %>%
    bind_rows(bootstrap())
}
```



```{r}

correlation_data %>%
  ggplot() +
  geom_density(aes(x=h1_scene, color="h1")) +
  geom_density(aes(x=null_scene, color="null")) +
  geom_vline(xintercept = mean(correlation_data$h1_scene), color="red") +
  geom_vline(xintercept = quantile(correlation_data$null_scene, 0.975), color="blue")
```


# get 40,000 correlation by bootstrapping
# 1) sample subjects with replacement
# 2) do the rest of analysis
# 3) repeat 1000 times

