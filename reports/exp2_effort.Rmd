---
title: "Exp2: Effort"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
 \usepackage{booktabs}
 \usepackage{longtable}
 \usepackage{array}
 \usepackage{multirow}
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
---

# Setup

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H',
                      echo=TRUE, warning=FALSE, message=FALSE)

```
```{r , include=FALSE}

# install.packages("tidyverse")
# install.packages("boot")
library(tidyverse)
library(boot)

th <- theme_classic()
theme_set(th)
```


```{r load data, echo=TRUE, results='hide'}
subject_data <- read_csv("../data/exp1/exp1_difficulty_parsed_trials.csv") %>%
  select(-c(WID)) %>%
  replace_na(list(response_frame = Inf)) %>%
  group_by(ID) %>%
  mutate(effort = scale(difficulty))

exp_design <- subject_data %>%
group_by(scene) %>%
  summarise(across(c(vel, n_dist), first))

model_att <- read_csv("../data/exp1/exp1_difficulty_target_designation_att.csv")
model_perf <- read_csv("../data/exp1/exp1_difficulty_target_designation_perf.csv")

```

```{r filter subjects, echo=FALSE, results='hide'}
td_by_subj_tracker <- subject_data %>%
  group_by(ID) %>%
  rowwise() %>%
  mutate(td = mean(c(td_1, td_2, td_3, td_4)))

td_by_subj <- td_by_subj_tracker %>%
  group_by(ID) %>%
  summarise(td_acc_mu = mean(td),
            n = n(),
            td_acc_se = sd(td) / sqrt(n),
            passed = td_acc_mu > 0.50 + 3 * td_acc_se) # chance performance lower than 0.5?
td_by_subj

passed_id <- td_by_subj %>%
  filter(passed) %>%
  select(ID)

good_td_by_subj_tracker <- passed_id %>%
  left_join(td_by_subj_tracker)

n_passed = sum(td_by_subj$passed)


good_td_by_scene_tracker <- good_td_by_subj_tracker %>%
  select(-td) %>% # remove trial perf across trackers for each subj
  group_by(scene) %>% # average across subjects
  summarise(
    n = n(),
    across(c(starts_with("td"), effort), list(mu = mean, sd = sd)),
    )


# separated for bootstrapping analysis later
good_subj_point_long <- good_td_by_subj_tracker %>%
  select(scene, contains("td_")) %>%
  pivot_longer(-scene,
               names_to = c(NA, "tracker"),
               names_sep = "_",
               values_to = "td")

good_td_by_scene <- good_subj_point_long %>%
  group_by(scene) %>%
  summarise(across(td, list(mu = mean, sd = sd))) %>%
  left_join(good_td_by_scene_tracker, by="scene") %>%
  left_join(exp_design, by="scene")

model_compute_by_scene <- model_att %>%
  group_by(scene, chain) %>%
  summarise(arousal = sum(cycles)) %>%
  group_by(scene) %>%
  summarize(arousal = mean(arousal)) %>%
  ungroup()

model_perf_by_scene <- model_perf %>%
  group_by(scene) %>%         # average across chains
  summarise(td_model = mean(td_acc)) %>%
  ungroup()
  # mutate(compute = scale(log(cycles_mu + 0.1)))

model_cov_by_scene <- model_compute_by_scene %>%
  left_join(model_perf_by_scene)

good_full_data <- good_td_by_scene %>%
  left_join(model_cov_by_scene) %>%
  mutate(diff_td = td_mu - td_model)
```
# Analysis

## Psychophysics summary


Below is a histogram describing the trial-average difficulty binned by velocity and number of distractors.
```{r subject accuracy and effort heatmaps}
x_breaks <- seq(2.0,14.0,length.out=14)
y_breaks <- seq(4,8,length.out=6)

breaks<-list(x=x_breaks, y=y_breaks)

good_td_by_scene %>%
  ggplot(aes(x=vel, y=n_dist, z=effort_mu)) +
  stat_summary_2d(breaks=breaks) +
  ggtitle("effort ~ velocity + distractors")
good_td_by_scene %>%
  ggplot(aes(x=vel, y=n_dist, z=td_mu)) +
  stat_summary_2d(breaks=breaks) +
  ggtitle("accuracy ~ velocity + distractors")
```

Here we show that the design variables explain an overwhelming amount of variance in tracking accuracy and difficulty ratings. 
```{r univariate experimental design}
good_td_by_scene %>%
  with(lm(td_mu ~ n_dist + vel)) %>%
  summary()

good_td_by_scene %>%
  with(lm(effort_mu ~ n_dist + vel)) %>%
  summary()
```


## Comparing human and model performance


This shows that the model's generative model and attention parameters are tuned
to match overall tracking performance of humans (~73%).

```{r subject and model accuracy histograms}
good_full_data %>%
  pivot_longer(cols=c("td_mu", "td_model"), names_to="organism", values_to="acc") %>%
  ggplot(aes(x=acc)) +
  facet_grid(vars(organism)) +
  geom_histogram() + 
  ggtitle("Distribution of tracking accuracy for humans and model")

good_full_data %>%
  summarise(
    human_td = mean(td_mu),
    model_td = mean(td_model)
  )

good_full_data %>%
  ggplot(aes(diff_td)) +
  geom_histogram() + 
  ggtitle("Distribution of trial-level differences in accuracy")

good_full_data %>%
  ggplot(aes(x = vel, y = n_dist, z = diff_td)) +
  scale_fill_gradient2() +
  stat_summary_2d(breaks=breaks) +
  ggtitle("diff in acc ~ velocity + distractors")

good_full_data %>%
  ggplot(aes(x=vel, y=n_dist, z=td_mu)) +
  stat_summary_2d(breaks=breaks) +
  ggtitle("human acc ~ velocity + distractors")

good_full_data %>%
ggplot(aes(x=vel, y=n_dist, z=td_model)) +
  stat_summary_2d(breaks=breaks) +
  ggtitle("model acc ~ velocity + distractors")
```

```{r univariate accuracy}
perf_model <- good_full_data %>%
  with(lm(td_mu ~ td_model))
perf_model %>%
  summary()

good_full_data %>%
  ggplot(aes(x = td_model, 
             y = td_mu)) +
  geom_point(color = "#5aa67b", size = 3.1) + 
  geom_smooth(method = "lm", color = "black") +
  theme_classic() + 
  theme(legend.position = "none",
        axis.ticks = element_blank(),
        axis.title = element_blank())

```
```{r bootstrap accuracy}
set.seed(0)

univariate_acc <- function(data, indices) {
  d = data[indices,] # sample trials with replacement 
  fit <- d %>% with(lm(td_mu ~ td_model)) %>% summary
  result <- fit$r.squared
  return(result) #return explained variance
}

univ_boostrap_steps = 10000

reps <- boot(data=good_full_data, 
             statistic=univariate_acc,
             R=univ_boostrap_steps, 
             ncpus = 8,
             parallel = "multicore")
#view results of boostrapping
reps
plot(reps)
# bootstrap percentiles
univ_cis_td <- boot.ci(reps, type="all")
univ_cis_td
univ_cis_td$mu <- mean(reps$t)
```

### Explaining effort ratings with arousal

Similar to tracking performance, cumulative arousal corresponds to human effort ratings. 
```{r}
good_full_data %>%
  mutate(zarousal = scale(arousal)) %>%
  pivot_longer(cols=c("effort_mu", "zarousal"), names_to="organism", values_to="effort") %>%
  ggplot(aes(x=effort)) +
  facet_grid(vars(organism)) +
  geom_histogram(bins = 20) + 
  ggtitle("Distribution of effort for humans and model")

good_full_data %>%
  ggplot(aes(x=vel, y=n_dist, z=effort_mu)) +
  stat_summary_2d(breaks=breaks) +
  ggtitle("effort ~ velocity + distractors")

good_full_data %>%
  ggplot(aes(x=vel, y=n_dist, z=scale(arousal))) +
  stat_summary_2d(breaks=breaks) +
  ggtitle("arousal ~ velocity + distractors")


effort_cycles <-good_full_data %>%
  with(lm(effort_mu  ~ arousal)) 

effort_cycles %>%
  summary()
```

```{r Fig 3B}
good_full_data %>%
  ggplot(aes(x=scale(arousal), y=effort_mu)) +
  geom_point(color = "#5aa67b", size = 3.1) + 
  geom_smooth(method = "lm", color = "black") +
  theme_classic() + 
  theme(legend.position = "none",
        axis.ticks = element_blank(),
        axis.title = element_blank())

```

```{r univ effort bootstrap}
set.seed(0)

univariate_effort <- function(data, indices) {
  d = data[indices,] # sample trials with replacement 
  arousal_fit <- d %>% with(lm(effort_mu ~ arousal)) %>% summary
  exp_fit <- d %>% with(lm(effort_mu ~ vel + n_dist)) %>% summary
  subj_td_fit <- d %>% with(lm(effort_mu ~ td_mu)) %>% summary
  model_td_fit <- d %>% with(lm(effort_mu ~ td_model)) %>% summary
  result <- c(arousal_fit$r.squared,
              exp_fit$r.squared,
              subj_td_fit$r.squared,
              model_td_fit$r.squared)
  return(result) #return explained variance
}

reps <- boot(data=good_full_data, 
             statistic=univariate_effort,
             R=univ_boostrap_steps, 
             ncpus = 8,
             parallel = "multicore")
#view results of boostrapping
reps
plot(reps)
# CIs
univ_effort_arousal <- boot.ci(reps, type="all", index = 1)
univ_effort_arousal$mu <- mean(reps$t[,1])
univ_effort_exp <- boot.ci(reps, type="all", index = 2)
univ_effort_exp$mu <- mean(reps$t[,2])
univ_effort_subj_td <- boot.ci(reps, type="all", index = 3)
univ_effort_subj_td$mu <- mean(reps$t[,3])
univ_effort_model_td <- boot.ci(reps, type="all", index = 4)
univ_effort_model_td$mu <- mean(reps$t[,4])
```

### Partial regression analysis

In order to control for external factors influencing effort ratings, we perform a series of partial regressions on candidate factors.


```{r test definition}
set.seed(0)

resid_r.squared <- function(covariate, predictor, data) {
  res_pred <- lm(as.formula(paste(predictor, " ~ ", covariate)),
                  data = data)
  res_hr <- lm(as.formula(paste("effort_mu ~ ", covariate)),
                  data = data)
  res_data <- data.frame(pred = res_pred$residuals,
                         dep = res_hr$residuals)
  r <- cor.test(~ dep + pred, data = res_data)
  return(c((r$estimate)^2, r$estimate < 0.0))
}

residual_test <- function(data, indices) {
  # sample trials with replacement
  d = data[indices,]
  
  res_exp <- resid_r.squared("vel + n_dist", "arousal", d)
  res_subj_td <- resid_r.squared("td_mu", "arousal", d)
  res_model_td <- resid_r.squared("td_model", "arousal", d)
  result <- c(res_exp, res_subj_td, res_model_td)
  return(result) #return R2 of model
}

res_bootstrap_steps = 5000
```

```{r Results}
reps <- boot(data=good_full_data, 
             statistic=residual_test, 
             R=res_bootstrap_steps, 
             ncpus = 8,  parallel = "multicore")
# visualizing samples for
# arousal after residualizing:
plot(reps, 1) # experimental design
plot(reps, 3) # subject accuracy
plot(reps, 5) # model accuracy
# calculating CIs and pvalues
res_exp <- boot.ci(reps, type="perc", index=1) 
res_exp$mu <- mean(reps$t[, 1])
res_exp_p_val <- sum(reps$t[, 2]) / reps$R 
res_subj_td <- boot.ci(reps, type="perc", index=3) 
res_subj_td$mu <- mean(reps$t[, 3])
res_subj_td_p_val <- sum(reps$t[, 4]) / reps$R 
res_model_td <- boot.ci(reps, type="perc", index=5)
res_model_td$mu <- mean(reps$t[, 5])
res_model_td_p_val <- sum(reps$t[, 6]) / reps$R 

# aggregrating results
resid_results <-data.frame(
                           covariate = c("exp", "subj_td", "model_td"),
                           r.squared = c(res_exp$mu,
                                         res_subj_td$mu,
                                         res_model_td$mu),
                           conf.low = c(res_exp$perc[4],
                                        res_subj_td$perc[4],
                                        res_model_td$perc[4]),
                           conf.high= c(res_exp$perc[5],
                                        res_subj_td$perc[5], 
                                        res_model_td$perc[5]),
                           p.val =    c(res_exp_p_val, 
                                        res_subj_td_p_val,
                                        res_model_td_p_val)
                           )

resid_results
```

```{r Fig 3C}

resid_results %>%
  # filter(covariate != "exp") %>%
  ggplot(aes(x = reorder(covariate, r.squared), y = r.squared)) + 
  geom_col(width = 0.5, fill = "#5aa67b")+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.075, size = 0.7) + 
  # ylim(0., 0.8) +
  theme(legend.position = "none",
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        # axis.text = element_blank(),
        aspect.ratio = 1.3
        )
```

Here we address the possibility that effort ratings are primarily driven by subject's instrospectively perceived accuracy and instead reflect a measure of confidence rather than effort. To remove shared variance with human accuracy we performed partial regressions on human effort and model arousal.
A moderate but significant amount of residualized variance in difficulty was explained by residualized variance in allocated cycles.
It is important to note that this residualized $R^2$ approaches the upper limit of possible variance explained given the stregnth of the first stage. 


# Noise-level

Using this [guide](https://www.statology.org/bootstrapping-in-r/)


## Trial level bootstrapping
```{r  split half correlation}
subj_data_nested <- passed_id %>%
  left_join(td_by_subj_tracker, by = "ID") %>%
  ungroup() %>%
  nest_by(ID)

effort_by_group <- function(group, gname) {
  d <- group %>%
    unnest(cols = c(ID, data)) %>%
    # select(-td) %>% # remove trial perf across trackers for each subj
    group_by(scene) %>% # average across subjects
    summarise(effort = mean(effort),
              td  = mean(td)) %>%
    mutate(group = gname)
  return (d)
}

splithalf_cor <- function(data, indices) {
  splits <- split(indices, 
                  cut(seq_along(indices),  
                      2,
                      labels = FALSE))
  a <- data[unlist(splits[1]),] %>%
    effort_by_group("a") %>%
    select(-td)
  
  b <-data[unlist(splits[2]),] %>%
    effort_by_group("b") %>%
    select(-td)

  df <- rbind(a, b) %>%
    pivot_wider(names_from = group,
                values_from = effort)
  fit <- df %>% with(lm(a ~ b)) %>% summary
  return (fit$r.square) #return R-squared of model
}

reps <- boot(data = subj_data_nested, 
             statistic=splithalf_cor,
             sim = "permutation",
             R=5000,
             ncpus = 8, 
             parallel = "multicore")

#view results of boostrapping
reps
plot(reps)
cis_split_half <- boot.ci(reps, type="perc")
cis_split_half
mean(reps$t)
```

# Subject bootstrapping

```{r test definition}
set.seed(0)

subj_data_nested <- passed_id %>%
  left_join(td_by_subj_tracker, by = "ID") %>%
  ungroup() %>%
  nest_by(ID)


resid_r.squared <- function(covariate, predictor, data) {
  res_pred <- lm(as.formula(paste(predictor, " ~ ", covariate)),
                  data = data)
  res_hr <- lm(as.formula(paste("effort ~ ", covariate)),
                  data = data)
  res_data <- data.frame(pred = res_pred$residuals,
                         dep = res_hr$residuals)
  r <- cor.test(~ dep + pred, data = res_data)
  return(c((r$estimate)^2, r$estimate < 0.0))
}

residual_test <- function(data, indices) {
  # sample trials with replacement
  d = data[indices,] %>%
    effort_by_group("a") %>%
    left_join(model_cov_by_scene, by = "scene") %>%
    left_join(exp_design, by = "scene")

  res_exp <- resid_r.squared("vel + n_dist", "arousal", d)
  res_subj_td <- resid_r.squared("td", "arousal", d)
  res_model_td <- resid_r.squared("td_model", "arousal", d)
  result <- c(res_exp, res_subj_td, res_model_td)
  return(result) #return R2 of model
}

res_bootstrap_steps = 5000
```

```{r Results}
reps <- boot(data=subj_data_nested, 
             statistic=residual_test, 
             R=res_bootstrap_steps, 
             ncpus = 8,  parallel = "multicore")
# visualizing samples for
# arousal after residualizing:
plot(reps, 1) # experimental design
plot(reps, 3) # subject accuracy
plot(reps, 5) # model accuracy
# calculating CIs and pvalues
res_exp <- boot.ci(reps, type="perc", index=1) 
res_exp$mu <- mean(reps$t[, 1])
res_exp_p_val <- sum(reps$t[, 2]) / reps$R 
res_subj_td <- boot.ci(reps, type="perc", index=3) 
res_subj_td$mu <- mean(reps$t[, 3])
res_subj_td_p_val <- sum(reps$t[, 4]) / reps$R 
res_model_td <- boot.ci(reps, type="perc", index=5)
res_model_td$mu <- mean(reps$t[, 5])
res_model_td_p_val <- sum(reps$t[, 6]) / reps$R 

# aggregrating results
resid_results <-data.frame(
                           covariate = c("exp", "subj_td", "model_td"),
                           order = c(3, 1, 2),
                           r.squared = c(res_exp$mu,
                                         res_subj_td$mu,
                                         res_model_td$mu),
                           conf.low = c(res_exp$perc[4],
                                        res_subj_td$perc[4],
                                        res_model_td$perc[4]),
                           conf.high= c(res_exp$perc[5],
                                        res_subj_td$perc[5], 
                                        res_model_td$perc[5]),
                           p.val =    c(res_exp_p_val, 
                                        res_subj_td_p_val,
                                        res_model_td_p_val)
                           )

resid_results
```

```{r Fig 3C}

resid_results %>%
  # filter(covariate != "exp") %>%
  ggplot(aes(x = reorder(covariate, order), y = r.squared)) + 
  geom_col(width = 0.75, fill = "#5aa67b")+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.175, size = 0.7) + 
  # ylim(0., 0.8) +
  theme(legend.position = "none",
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        # axis.text = element_blank(),
        aspect.ratio = 2.
        )
```
