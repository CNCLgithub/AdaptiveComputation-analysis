---
title: "Exp 1: Difficulty G(0.005)"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
 \usepackage{booktabs}
 \usepackage{longtable}
 \usepackage{array}
 \usepackage{multirow}
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
---

# Setup

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H',
                      echo=TRUE, warning=FALSE, message=FALSE)

```
```{r , include=FALSE}

# install.packages("tidyverse")
# install.packages("boot")
library(tidyverse)
library(boot)

th <- theme_classic()
theme_set(th)
```

## Load Data

```{r echo=TRUE, results='hide'}
subject_data <- read_csv("../data/exp1/exp1_difficulty_parsed_trials.csv") %>%
  select(-c(WID)) %>%
  replace_na(list(response_frame = Inf)) %>%
  group_by(ID) %>%
  mutate(difficulty = scale(difficulty))

exp_design <- subject_data %>%
group_by(scene) %>%
  summarise(across(c(vel, n_dist), first))

model_att <- read_csv("../data/exp1/exp1_difficulty_target_designation_att.csv")
model_perf <- read_csv("../data/exp1/exp1_difficulty_target_designation_perf.csv")

```

```{r, echo=FALSE, results='hide'}
td_by_subj_tracker <- subject_data %>%
  group_by(ID) %>%
  rowwise() %>%
  mutate(td = mean(c(td_1, td_2, td_3, td_4)))

td_by_subj <- td_by_subj_tracker %>%
  group_by(ID) %>%
  summarise(td_acc_mu = mean(td),
            n = n(),
            td_acc_se = sd(td) / sqrt(n),
            passed = td_acc_mu > 0.50 + 3 * td_acc_se) # chance performance lower than 0.5?
td_by_subj

passed_id <- td_by_subj %>%
  filter(passed) %>%
  select(ID)

good_td_by_subj_tracker <- passed_id %>%
  left_join(td_by_subj_tracker)

n_passed = sum(td_by_subj$passed)


good_td_by_scene_tracker <- good_td_by_subj_tracker %>%
  select(-td) %>% # remove trial perf across trackers for each subj
  group_by(scene) %>% # average across subjects
  summarise(
    n = n(),
    across(c(starts_with("td"), difficulty), list(mu = mean, sd = sd)),
    )


# separated for bootstrapping analysis later
good_subj_point_long <- good_td_by_subj_tracker %>%
  select(scene, contains("td_")) %>%
  pivot_longer(-scene,
               names_to = c(NA, "tracker"),
               names_sep = "_",
               values_to = "td")

good_td_by_scene <- good_subj_point_long %>%
  group_by(scene) %>%
  summarise(across(td, list(mu = mean, sd = sd))) %>%
  left_join(good_td_by_scene_tracker, by="scene") %>%
  left_join(exp_design, by="scene")

model_compute_by_scene <- model_att %>%
  group_by(scene, chain) %>%
  summarise(cycles = sum(cycles)) %>%
  group_by(scene) %>%
  summarize(cycles = mean(cycles)) %>%
  ungroup()

model_perf_by_scene <- model_perf %>%
  group_by(scene) %>%         # average across chains
  summarise(td_model = mean(td_acc)) %>%
  ungroup()
  # mutate(compute = scale(log(cycles_mu + 0.1)))

model_cov_by_scene <- model_compute_by_scene %>%
  left_join(model_perf_by_scene)
```

## Psychophysics summary


Below is a histogram describing the trial-average difficulty binned by velocity and number of distractors.
The violin plots show the marginals with respect to each design variable. 
```{r}
x_breaks <- seq(2.0,14.0,length.out=14)
y_breaks <- seq(4,8,length.out=6)

breaks<-list(x=x_breaks, y=y_breaks)

good_td_by_scene %>%
  ggplot(aes(x=vel, y=n_dist, z=difficulty_mu)) +
  stat_summary_2d(breaks=breaks) +
  ggtitle("difficulty ~ velocity + distractors")

good_td_by_scene %>%
  ggplot(aes(factor(n_dist), y=difficulty_mu)) +
  geom_violin() + 
  ggtitle("difficulty ~ distractors")


good_td_by_scene %>%
  ggplot(aes(factor(vel), y=difficulty_mu)) +
  geom_violin() + 
  ggtitle("difficulty ~ velocity")


```

Here we show that the design variables explain an overwhelming amount of variance in tracking accuracy and difficulty ratings. 
```{r}
good_td_by_scene %>%
  with(lm(td_mu ~ n_dist + vel)) %>%
  summary()

good_td_by_scene %>%
  with(lm(difficulty_mu ~ n_dist + vel)) %>%
  summary()
```

## Modeling psychophysics

< Correctly compute sd across chains >

> across(c(starts_with("td"), difficulty), list(mu = mean, sd = sd)),

```{r, echo = FALSE, results = 'hide'}
# model_td_compute_by_scene <- model_data %>%
#   select(-id) %>%
#   rowwise() %>%
#   mutate(td_chain = mean(c(tracker_1, tracker_2, tracker_3, tracker_4))) %>%
#   group_by(scene) %>%         # average across chains
#   summarise(across(-chain, list(mu = mean, sd = sd))) %>%
#   rename(td_model = td_chain_mu) %>%
#   ungroup() %>%
#   mutate(compute = scale(log(cycles_mu + 0.1)))

  
good_full_data <- good_td_by_scene %>%
  left_join(model_cov_by_scene) %>%
  mutate(diff_td = td_mu - td_model)

```


### Comparing performance and human accuracy


Here we show the distribution of accuracy for each organism as well as their average performance and distribution of trial-level differences in performance. 

Clearly, the model outperforms humans in a range of trials (TODO: look at relationship with design variables).
Interestingly, the spread of performance across trials is qualitatively distinct for humans.
While the model exhibits a tialed spread, humans clearely have three modes of performance, around $85\%, 72\%, 55\%$, respectively. 
I believe these modes are signatures of representational dropping conferring an advantage in the form a resilient lower performance bound. 
Rather than approaching chance (which can be as low as $13\%$ in some of the harder trials), humans drop trackers, merging them with ensembles, and focusing more cycles on the remaining trackers. 
Note that merging provides a principled manner for the experience of having high certainty of the target status of two objects while being almost completely clueless regarding the rest without believing that the other two targets "dissappeared". 
I'd imagine that we varied the number of targets across trials (2-5), subjects would still be able to 1. track some subset of targets and 2. accurately report how many targets are in the scene regardless of performance in 1. 
```{r}
good_full_data %>%
  pivot_longer(cols=c("td_mu", "td_model"), names_to="organism", values_to="acc") %>%
  ggplot(aes(x=acc)) +
  facet_grid(vars(organism)) +
  geom_histogram() + 
  ggtitle("Distribution of tracking accuracy for humans and model")

good_full_data %>%
  summarise(
    human_td = mean(td_mu),
    model_td = mean(td_model)
  )

good_full_data %>%
  ggplot(aes(diff_td)) +
  geom_histogram() + 
  ggtitle("Distribution of trial-level differences in accuracy")

good_full_data %>%
  ggplot(aes(x = vel, y = n_dist, z = diff_td)) +
  scale_fill_gradient2() +
  stat_summary_2d(breaks=breaks) +
  ggtitle("diff in acc ~ velocity + distractors")

good_full_data %>%
ggplot(aes(x=vel, y=n_dist, z=td_mu)) +
  stat_summary_2d(breaks=breaks) +
  ggtitle("human acc ~ velocity + distractors")

good_full_data %>%
ggplot(aes(x=vel, y=n_dist, z=td_model)) +
  stat_summary_2d(breaks=breaks) +
  ggtitle("model acc ~ velocity + distractors")
```

> TODO: Look at these plots at the tracker level

Despite these differences in accuracy, model accuracy is still able to explain a strong portion of the variance in human performance
```{r}
good_full_data %>%
  with(lm(td_mu ~ td_model)) %>%
  summary()

good_full_data %>%
  ggplot(aes(x = td_model, 
             y = td_mu)) +
  geom_point()


```


### Model compute explaining difficulty

Similar to tracking performance, total amount of allocated resources explains a strong amount of variance in human difficulty ratings. 
```{r}
good_full_data %>%
  mutate(zcycles = scale(cycles)) %>%
  pivot_longer(cols=c("difficulty_mu", "zcycles"), names_to="organism", values_to="effort") %>%
  ggplot(aes(x=effort)) +
  facet_grid(vars(organism)) +
  geom_histogram() + 
  ggtitle("Distribution of effort for humans and model")

good_full_data %>%
  with(lm(difficulty_mu ~ cycles)) %>%
  summary()

good_full_data %>%
  ggplot(aes(x=cycles, y=difficulty_mu)) +
  geom_point(color = "#5aa67b", size = 3.1) + 
  geom_smooth(method = "lm", color = "black") +
  theme_classic() 

```

In order to control for external factors influencing effort ratings, we perform a series of partial regressions on candidate factors.

Here we address the possibility that effort ratings are primarily driven by subject's instrospectively perceived accuracy and instead reflect a measure of confidence rather than effort. To remove shared variance with human accuracy we performed partial regressions on difficulty and allocated cycles.
A moderate but significant amount of residualized variance in difficulty was explained by residualized variance in allocated cycles.
It is important to note that this residualized $R^2$ approaches the upper limit of possible variance explained given the stregnth of the first stage. 
```{r}
good_full_data %>%
  ggplot(aes(x=td_mu, y=difficulty_mu)) +
  geom_point()

good_full_data %>%
  with(lm(difficulty_mu ~ td_mu)) %>%
  summary()

pred_compute <- good_full_data %>%
  with(lm(cycles ~ td_mu)) %>%
  predict()
pred_difficulty <- good_full_data %>%
  with(lm(difficulty_mu ~ td_mu)) %>%
  predict()

good_full_data <- good_full_data %>%
  mutate(res_difficulty = difficulty_mu - pred_difficulty,
         res_compute = cycles - pred_compute)

model <- good_full_data %>%
  with(lm(res_difficulty ~ res_compute))

pred_pred_difficulty <- model %>% 
  predict()

good_full_data <- good_full_data %>%
  mutate(res_res_difficulty = res_difficulty - pred_pred_difficulty,
         abs_res_res_diff = abs(res_res_difficulty))

model %>%
  summary()

good_full_data %>%
  ggplot(aes(x = res_compute, 
             y = res_difficulty)) +
  geom_point() 
```

# ```{r}
# pred_compute <- good_full_data %>%
#   with(lm_robust(td ~ compute)) %>%
#   predict()
# pred_difficulty <- good_full_data %>%
#   with(lm_robust(difficulty ~ compute)) %>%
#   predict()
# 
# good_full_data <- good_full_data %>%
#   mutate(res_difficulty = difficulty - pred_difficulty,
#          res_compute = compute - pred_compute)
# 
# model <- good_full_data %>%
#   with(lm_robust(res_difficulty ~ res_compute))
# 
# pred_pred_difficulty <- model %>% 
#   predict()
# 
# good_full_data <- good_full_data %>%
#   mutate(res_res_difficulty = res_difficulty - pred_pred_difficulty,
#          abs_res_res_diff = abs(res_res_difficulty))
# 
# model %>%
#   summary()
# 
# good_full_data %>%
#   ggplot(aes(x = vel, y = n_dist, z = res_res_difficulty)) +
#   stat_summary_2d(breaks=breaks) +
#   ggtitle("res res ~ velocity + distractors")
# ```

To explore the possiblity that our model is able to explain effort above and beyond a linear relationshop among design variables (velocity and number of distractors), we performed another partial regression analysis, removed shared variance of difficulty and allocated cycles with velocity and number of distractors. 
Similar to before, a moderate but significant relationship survives and is at the upper limit of residual variance. 
```{r}
pred_compute <- good_full_data %>%
  with(lm(cycles ~ vel + n_dist)) %>%
  predict()
pred_difficulty <- good_full_data %>%
  with(lm(difficulty_mu ~ vel + n_dist)) %>%
  predict()

good_full_data <- good_full_data %>%
  mutate(res_difficulty = difficulty_mu - pred_difficulty,
         res_compute = cycles - pred_compute)

good_full_data %>%
  with(lm(res_difficulty ~ res_compute)) %>%
  summary()

good_full_data %>%
  ggplot(aes(x=res_compute, y=res_difficulty)) +
  geom_point() + 
  geom_smooth(method="lm")
```

# Bootstrapping

Using this [guide](https://www.statology.org/bootstrapping-in-r/)

first define the r2 func


## Trial level bootstrapping
```{r}
set.seed(0) # TODO: Should we set this seed?

rsq_function <- function(formula, data, indices) {
  d <- data[indices,] #allows boot to select sample
  fit <- lm(formula, data=d) #fit regression model
  return(summary(fit)$r.square) #return R-squared of model
}

reps <- boot(data=good_full_data, statistic=rsq_function, R=2000, formula=difficulty_mu ~ compute)

#view results of boostrapping
reps

plot(reps)

#calculate adjusted bootstrap percentile (BCa) interval
boot.ci(reps, type="bca")

```

## Subject level bootstrapping
```{r}
set.seed(0) # TODO: Should we set this seed?

subj_data_nested <- passed_id %>%
  left_join(td_by_subj_tracker, by = "ID") %>%
  ungroup() %>%
  nest_by(ID)

ate_function <- function(d) {
  ate <- d %>% 
    unnest(cols = c(ID, data)) %>%
    group_by(scene) %>% # average across subjects
    summarise(
      n = n(),
      across(c(td, difficulty), list(mu = mean, sd = sd)),
      .groups = "keep",
      ) %>%
    left_join(exp_design, by = "scene") %>%
    left_join(model_td_compute_by_scene, by = "scene") %>%
    ungroup
  return (ate)
}

rsq_function <- function(formula, data, indices) {
  d <- data[indices,] #allows boot to select sample of subjects
  # average over subject points
  full_d <- d %>% 
    ate_function
  fit <- lm(formula, data = full_d)
  return(summary(fit)$r.square) #return R-squared of model
}
```


`Effort ~ Compute`

```{r}
reps <- boot(data=subj_data_nested, statistic=rsq_function, R=2000, formula=difficulty_mu ~ compute)
#view results of boostrapping
reps
plot(reps)
#calculate adjusted bootstrap percentile (BCa) interval
cis_effort <- boot.ci(reps, type="bca")
```

`Effort ~ Model Acc`

```{r}
reps <- boot(data=subj_data_nested, statistic=rsq_function, R=2000, formula=difficulty_mu ~ td_model)
#view results of boostrapping
reps
plot(reps)
#calculate adjusted bootstrap percentile (BCa) interval
cis_effort_td_model <- boot.ci(reps, type="bca")
```
`Effort ~ Human Acc`

```{r}
reps <- boot(data=subj_data_nested, statistic=rsq_function, R=2000, formula=difficulty_mu ~ td_mu)
#view results of boostrapping
reps
plot(reps)
#calculate adjusted bootstrap percentile (BCa) interval
cis_effort_td_mu <- boot.ci(reps, type = "bca")
```

`Human Acc ~ Model Acc.` (SI)

```{r}
reps <- boot(data=subj_data_nested, statistic=rsq_function, R=1000, formula=td_mu ~ td_model)
#view results of boostrapping
reps
plot(reps)
#calculate adjusted bootstrap percentile (BCa) interval
cis_human_acc <- boot.ci(reps, type="bca")
```

Partial regression 

```{r}
rsq_function <- function(first, second, data, indices) {
  d <- data[indices,] #allows boot to select sample of subjects
  # average over subject points
  full_d <- d %>% 
    ate_function
  # partial regressions
  first_ind <- lm(as.formula(paste(first, " ~ ", second)),
                  data = full_d)
  first_dep <- lm(as.formula(paste("difficulty_mu ~ ", second)),
                  data = full_d)
  fit <- lm(as.formula(paste("y ~ x")),
            data = data.frame(y = first_dep$residuals,
                              x = first_ind$residuals))
  return(summary(fit)$r.square) #return R-squared of model
}

```
```{r}
reps <- boot(data=subj_data_nested, statistic=rsq_function, R=1000, first = "compute", second = "td_mu")
#view results of boostrapping
reps
plot(reps)
#calculate adjusted bootstrap percentile (BCa) interval
cis_effort_compute_sans_human_acc <- boot.ci(reps, type="bca")
```

```{r}
reps <- boot(data=subj_data_nested, statistic=rsq_function, R=1000, first = "compute", second = "td_model")
#view results of boostrapping
reps
plot(reps)
#calculate adjusted bootstrap percentile (BCa) interval
cis_effort_compute_sans_model_acc <- boot.ci(reps, type="bca")
```

Split-half correlation


```{r}
rsq_function <- function(data, indices) {
  
  splits <- split(indices, 
                  cut(seq_along(indices),  
                      2,
                      labels = FALSE))
  a <- data[unlist(splits[1]),] %>%
    ate_function %>%
    select(scene, difficulty_mu) %>%
    rename(a = difficulty_mu)
  
  b <-data[unlist(splits[2]),] %>%
    ate_function %>%
    select(scene, difficulty_mu) %>%
    rename(b = difficulty_mu)
  
  sample <- a %>%
    left_join(b, by = "scene")
  
  # sample_ate %>% head %>% print
  
  fit <- lm(b ~ a, data = sample)
  return(summary(fit)$r.square) #return R-squared of model
}

reps <- boot(data=subj_data_nested, statistic=rsq_function, R=1000)
#view results of boostrapping
reps
plot(reps)
#calculate adjusted bootstrap percentile (BCa) interval
cis_split_half <- boot.ci(reps, type="bca")
```


## Figure 4D


First order effects

```{r}

fig_4D_df <- data.frame(model = c("1", "2", "3"),
                        conf.low = c(cis_effort$bca[4], 
                                     cis_effort_td_model$bca[4], 
                                     cis_effort_td_mu$bca[4]),
                        conf.high = c(cis_effort$bca[5], 
                                      cis_effort_td_model$bca[5], 
                                      cis_effort_td_mu$bca[5])) %>%
  mutate(r2 = 0.5 * (conf.low + conf.high))
fig_4D <- function(data) {
  data %>%
    ggplot(aes(x = model, y = r2, fill = model)) + 
    # geom_hline(yintercept = cis_split_half$bca[5],
    #            linetype = "dashed",
    #            size = 1.0) +
    geom_col(width = 0.6) +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.1, size = 0.7) +

    theme(legend.position = "none",
          axis.ticks = element_blank(),
          axis.title = element_blank(),
          # axis.text = element_blank(),
          aspect.ratio = 1
          )
}
okabe <- c("#5aa67b", "#CC79A7", "#56B4E9", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
withr::with_options(
  list(ggplot2.discrete.fill = okabe),
  print(fig_4D(fig_4D_df))
)

```

Partial regressions



```{r}
fig_4D_df <- data.frame(model = c("1", "2"),
                        conf.low = c(cis_effort_compute_sans_model_acc$bca[4],
                                     cis_effort_compute_sans_human_acc$bca[4]),
                        conf.high = c(cis_effort_compute_sans_model_acc$bca[5],
                                     cis_effort_compute_sans_human_acc$bca[5])) %>%
  mutate(r2 = 0.5 * (conf.low + conf.high))

okabe <- c("#5aa6a1", "#5fa65a", "#56B4E9", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
withr::with_options(
  list(ggplot2.discrete.fill = okabe),
  print(fig_4D(fig_4D_df))
)

```
